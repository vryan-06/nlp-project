{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-23T10:44:20.206974Z",
     "iopub.status.busy": "2023-10-23T10:44:20.206609Z",
     "iopub.status.idle": "2023-10-23T10:44:20.730361Z",
     "shell.execute_reply": "2023-10-23T10:44:20.729264Z",
     "shell.execute_reply.started": "2023-10-23T10:44:20.206912Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T10:44:20.732983Z",
     "iopub.status.busy": "2023-10-23T10:44:20.732325Z",
     "iopub.status.idle": "2023-10-23T10:44:20.737167Z",
     "shell.execute_reply": "2023-10-23T10:44:20.736315Z",
     "shell.execute_reply.started": "2023-10-23T10:44:20.732655Z"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_DIR = \"./dataset/\"\n",
    "SAVE_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-10-23T10:44:20.738407Z",
     "iopub.status.busy": "2023-10-23T10:44:20.738176Z",
     "iopub.status.idle": "2023-10-23T10:44:21.242070Z",
     "shell.execute_reply": "2023-10-23T10:44:21.241347Z",
     "shell.execute_reply.started": "2023-10-23T10:44:20.738368Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score  \n",
       "0              8  \n",
       "1              9  \n",
       "2              7  \n",
       "3             10  \n",
       "4              8  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "      <td>11.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "      <td>16.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "      <td>13.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   domain1_score      score  \n",
       "0              8  13.333333  \n",
       "1              9  15.000000  \n",
       "2              7  11.666667  \n",
       "3             10  16.666667  \n",
       "4              8  13.333333  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X['score'] = scaler.fit_transform(X['domain1_score'].values.reshape(-1, 1))\n",
    "\n",
    "new_min = 0\n",
    "new_max = 100\n",
    "X['score'] = (X['score'] - X['score'].min()) / (X['score'].max() - X['score'].min()) * (new_max - new_min) + new_min\n",
    "y = np.round(X['score'])\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T10:44:21.405656Z",
     "iopub.status.busy": "2023-10-23T10:44:21.405429Z",
     "iopub.status.idle": "2023-10-23T10:44:21.412628Z",
     "shell.execute_reply": "2023-10-23T10:44:21.411899Z",
     "shell.execute_reply.started": "2023-10-23T10:44:21.405618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        13.0\n",
       "1        15.0\n",
       "2        12.0\n",
       "3        17.0\n",
       "4        13.0\n",
       "         ... \n",
       "12971    58.0\n",
       "12972    53.0\n",
       "12973    67.0\n",
       "12974    67.0\n",
       "12975    67.0\n",
       "Name: score, Length: 12976, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T11:11:52.952801Z",
     "iopub.status.busy": "2023-10-23T11:11:52.952440Z",
     "iopub.status.idle": "2023-10-23T11:11:55.459158Z",
     "shell.execute_reply": "2023-10-23T11:11:55.458129Z",
     "shell.execute_reply.started": "2023-10-23T11:11:52.952752Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    # index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec, model[word])       \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T11:11:55.461277Z",
     "iopub.status.busy": "2023-10-23T11:11:55.460988Z",
     "iopub.status.idle": "2023-10-23T11:11:59.482711Z",
     "shell.execute_reply": "2023-10-23T11:11:59.481987Z",
     "shell.execute_reply.started": "2023-10-23T11:11:55.461223Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from tensorflow.keras.models import Sequential, load_model, model_from_config\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(200, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 200], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-23T11:11:59.485130Z",
     "iopub.status.busy": "2023-10-23T11:11:59.484595Z",
     "iopub.status.idle": "2023-10-23T11:12:44.457913Z",
     "shell.execute_reply": "2023-10-23T11:12:44.456770Z",
     "shell.execute_reply.started": "2023-10-23T11:11:59.485076Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for essay in X['essay']:\n",
    "    corpus.append(essay_to_wordlist(essay, True))\n",
    "\n",
    "embedding_dict={}\n",
    "\n",
    "with open('./glove.6B/glove.6B.200d.txt','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word] = vectors\n",
    "        \n",
    "# with open('./glove.6B/glove.6B.100d.txt','r', encoding=\"utf8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vectors = np.asarray(values[1:],'float32')\n",
    "#         embedding_dict[word] = vectors\n",
    "\n",
    "# with open('./glove.6B/glove.6B.50d.txt','r', encoding=\"utf8\") as f:\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         word = values[0]\n",
    "#         vectors = np.asarray(values[1:],'float32')\n",
    "#         embedding_dict[word] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 50d Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_essays = X_train['essay']\n",
    "test_essays = X_test['essay']\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for essay in train_essays:\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "\n",
    "num_features = 50     \n",
    "model = embedding_dict\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "\n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "\n",
    "# Reshape train and test vectors to 3 dimensions (1 represents one timestep)\n",
    "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 50)             20200     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                29440     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,705\n",
      "Trainable params: 49,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "163/163 [==============================] - 15s 15ms/step - loss: 250.2146 - mae: 9.2113\n",
      "Epoch 2/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 187.6505 - mae: 8.1463\n",
      "Epoch 3/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 159.0637 - mae: 7.4478\n",
      "Epoch 4/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 144.3113 - mae: 7.1088\n",
      "Epoch 5/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 134.4599 - mae: 6.8326\n",
      "Epoch 6/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 122.4577 - mae: 6.4716\n",
      "Epoch 7/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 116.1409 - mae: 6.3099\n",
      "Epoch 8/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 111.8120 - mae: 6.1927\n",
      "Epoch 9/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 108.1207 - mae: 6.0917\n",
      "Epoch 10/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 101.3670 - mae: 5.8812\n",
      "Epoch 11/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 101.2386 - mae: 5.8570\n",
      "Epoch 12/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 98.4426 - mae: 5.7733\n",
      "Epoch 13/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 96.2812 - mae: 5.6844\n",
      "Epoch 14/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 94.7831 - mae: 5.6166\n",
      "Epoch 15/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 90.7813 - mae: 5.5124\n",
      "Epoch 16/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 89.8018 - mae: 5.4150\n",
      "Epoch 17/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 89.1006 - mae: 5.3858\n",
      "Epoch 18/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 85.5162 - mae: 5.2499\n",
      "Epoch 19/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 85.3308 - mae: 5.2500\n",
      "Epoch 20/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 83.0659 - mae: 5.1580\n",
      "Epoch 21/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 81.2621 - mae: 5.1005\n",
      "Epoch 22/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 79.3011 - mae: 5.0144\n",
      "Epoch 23/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 78.8874 - mae: 4.9894\n",
      "Epoch 24/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 77.4258 - mae: 4.9594\n",
      "Epoch 25/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 74.0923 - mae: 4.8468\n",
      "Epoch 26/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 71.8289 - mae: 4.7462\n",
      "Epoch 27/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 72.3215 - mae: 4.7465\n",
      "Epoch 28/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 72.0139 - mae: 4.7266\n",
      "Epoch 29/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 69.8261 - mae: 4.6474\n",
      "Epoch 30/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 70.7639 - mae: 4.6579\n",
      "Epoch 31/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 67.4490 - mae: 4.5788\n",
      "Epoch 32/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 66.2524 - mae: 4.4503\n",
      "Epoch 33/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 65.6282 - mae: 4.4854\n",
      "Epoch 34/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 68.4975 - mae: 4.5077\n",
      "Epoch 35/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 64.0666 - mae: 4.3856\n",
      "Epoch 36/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 67.0901 - mae: 4.4591\n",
      "Epoch 37/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 63.1630 - mae: 4.3590\n",
      "Epoch 38/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 63.2073 - mae: 4.2947\n",
      "Epoch 39/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 61.7130 - mae: 4.3002\n",
      "Epoch 40/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 62.1500 - mae: 4.3127\n",
      "Epoch 41/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 62.6389 - mae: 4.3036\n",
      "Epoch 42/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 58.2121 - mae: 4.2022\n",
      "Epoch 43/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 61.0570 - mae: 4.2294\n",
      "Epoch 44/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 61.1698 - mae: 4.2498\n",
      "Epoch 45/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 57.9779 - mae: 4.1335\n",
      "Epoch 46/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 60.2487 - mae: 4.1882\n",
      "Epoch 47/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 59.7827 - mae: 4.1701\n",
      "Epoch 48/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 55.2003 - mae: 4.0601\n",
      "Epoch 49/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 58.6699 - mae: 4.1081\n",
      "Epoch 50/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 60.7165 - mae: 4.1771\n",
      "Epoch 51/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 56.3322 - mae: 4.1225\n",
      "Epoch 52/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 56.9215 - mae: 4.0931\n",
      "Epoch 53/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 54.6252 - mae: 4.0000\n",
      "Epoch 54/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 56.1971 - mae: 4.0800\n",
      "Epoch 55/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 54.1232 - mae: 3.9937\n",
      "Epoch 56/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 55.3018 - mae: 4.0154\n",
      "Epoch 57/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 53.2090 - mae: 3.9765\n",
      "Epoch 58/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 55.7191 - mae: 3.9969\n",
      "Epoch 59/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 56.8717 - mae: 4.0252\n",
      "Epoch 60/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 52.3850 - mae: 3.9453\n",
      "Epoch 61/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 53.7959 - mae: 3.9591\n",
      "Epoch 62/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 50.2399 - mae: 3.8719\n",
      "Epoch 63/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 53.0394 - mae: 3.9342\n",
      "Epoch 64/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 51.7245 - mae: 3.8756\n",
      "Epoch 65/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 52.8161 - mae: 3.9010\n",
      "Epoch 66/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 51.5687 - mae: 3.9136\n",
      "Epoch 67/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 52.1212 - mae: 3.9105\n",
      "Epoch 68/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 53.5401 - mae: 3.9456\n",
      "Epoch 69/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 50.8569 - mae: 3.8678\n",
      "Epoch 70/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 52.4934 - mae: 3.9017\n",
      "Epoch 71/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 53.7541 - mae: 3.9247\n",
      "Epoch 72/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 52.7923 - mae: 3.9078\n",
      "Epoch 73/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 50.7932 - mae: 3.8737\n",
      "Epoch 74/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 51.8841 - mae: 3.8700\n",
      "Epoch 75/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 49.9725 - mae: 3.8370\n",
      "Epoch 76/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 50.5598 - mae: 3.8377\n",
      "Epoch 77/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 51.1660 - mae: 3.8790\n",
      "Epoch 78/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 50.3134 - mae: 3.8208\n",
      "Epoch 79/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 51.1841 - mae: 3.8547\n",
      "Epoch 80/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 50.1889 - mae: 3.8126\n",
      "Epoch 81/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 49.3270 - mae: 3.7851\n",
      "Epoch 82/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 48.6923 - mae: 3.8025\n",
      "Epoch 83/150\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 48.5063 - mae: 3.7729\n",
      "Epoch 84/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 49.2782 - mae: 3.8187\n",
      "Epoch 85/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 48.6539 - mae: 3.7640\n",
      "Epoch 86/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 48.6059 - mae: 3.7607\n",
      "Epoch 87/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 49.7636 - mae: 3.7979\n",
      "Epoch 88/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 49.6621 - mae: 3.8000\n",
      "Epoch 89/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 52.6058 - mae: 3.8410\n",
      "Epoch 90/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 48.0214 - mae: 3.7660\n",
      "Epoch 91/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 48.3025 - mae: 3.7392\n",
      "Epoch 92/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 50.6481 - mae: 3.7907\n",
      "Epoch 93/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 47.8868 - mae: 3.7676\n",
      "Epoch 94/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 47.2255 - mae: 3.7049\n",
      "Epoch 95/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 45.8339 - mae: 3.6902\n",
      "Epoch 96/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 48.4865 - mae: 3.7604\n",
      "Epoch 97/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 48.2090 - mae: 3.7297\n",
      "Epoch 98/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 48.4116 - mae: 3.7225\n",
      "Epoch 99/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 45.3122 - mae: 3.6638\n",
      "Epoch 100/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 48.2289 - mae: 3.7187\n",
      "Epoch 101/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 45.4174 - mae: 3.6594\n",
      "Epoch 102/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 49.2404 - mae: 3.7481\n",
      "Epoch 103/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 45.4543 - mae: 3.6894\n",
      "Epoch 104/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 47.6178 - mae: 3.6897\n",
      "Epoch 105/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 48.4716 - mae: 3.7134\n",
      "Epoch 106/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 46.0915 - mae: 3.6665\n",
      "Epoch 107/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 47.1852 - mae: 3.7022\n",
      "Epoch 108/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 46.4281 - mae: 3.6913\n",
      "Epoch 109/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 47.4308 - mae: 3.7003\n",
      "Epoch 110/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 45.2067 - mae: 3.6577\n",
      "Epoch 111/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 46.6154 - mae: 3.6773\n",
      "Epoch 112/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 44.8563 - mae: 3.6592\n",
      "Epoch 113/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 44.7126 - mae: 3.6266\n",
      "Epoch 114/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 45.8630 - mae: 3.6336\n",
      "Epoch 115/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 46.1831 - mae: 3.6449\n",
      "Epoch 116/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 46.6792 - mae: 3.6647\n",
      "Epoch 117/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 46.5251 - mae: 3.6088\n",
      "Epoch 118/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 46.8556 - mae: 3.6793\n",
      "Epoch 119/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 46.1663 - mae: 3.6608\n",
      "Epoch 120/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 44.1474 - mae: 3.5635\n",
      "Epoch 121/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 45.1853 - mae: 3.6057\n",
      "Epoch 122/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 45.3882 - mae: 3.6166\n",
      "Epoch 123/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 44.3609 - mae: 3.5821\n",
      "Epoch 124/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 44.8657 - mae: 3.6253\n",
      "Epoch 125/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 46.3888 - mae: 3.6449\n",
      "Epoch 126/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 44.3320 - mae: 3.5802\n",
      "Epoch 127/150\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 44.3925 - mae: 3.6267\n",
      "Epoch 128/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 43.2905 - mae: 3.5905\n",
      "Epoch 129/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 45.2952 - mae: 3.5884\n",
      "Epoch 130/150\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 45.2643 - mae: 3.6212\n",
      "Epoch 131/150\n",
      "163/163 [==============================] - 1s 6ms/step - loss: 44.3653 - mae: 3.5821\n",
      "Epoch 132/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 44.5815 - mae: 3.5824\n",
      "Epoch 133/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 44.0073 - mae: 3.6126\n",
      "Epoch 134/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 44.3207 - mae: 3.5785\n",
      "Epoch 135/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 45.4060 - mae: 3.6411\n",
      "Epoch 136/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 41.1389 - mae: 3.5000\n",
      "Epoch 137/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 42.7229 - mae: 3.5299\n",
      "Epoch 138/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 45.8127 - mae: 3.6049\n",
      "Epoch 139/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 42.2232 - mae: 3.5199\n",
      "Epoch 140/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 44.0440 - mae: 3.5719\n",
      "Epoch 141/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 43.3696 - mae: 3.5460\n",
      "Epoch 142/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 44.1711 - mae: 3.5562\n",
      "Epoch 143/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 43.7223 - mae: 3.5039\n",
      "Epoch 144/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 43.9711 - mae: 3.5721\n",
      "Epoch 145/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 45.8340 - mae: 3.6093\n",
      "Epoch 146/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 43.0443 - mae: 3.5290\n",
      "Epoch 147/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 43.2908 - mae: 3.5432\n",
      "Epoch 148/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 42.4399 - mae: 3.5119\n",
      "Epoch 149/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 46.7641 - mae: 3.6435\n",
      "Epoch 150/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 44.3878 - mae: 3.5538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e6c3b2ce0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = get_model()\n",
    "lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 3ms/step\n",
      "Kappa Score: 0.9236359418268769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "lstm_model.save('./final_lstm_model50.h5')\n",
    "\n",
    "y_pred = lstm_model.predict(testDataVecs)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 100d Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_essays = X_train['essay']\n",
    "test_essays = X_test['essay']\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for essay in train_essays:\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "\n",
    "num_features = 100     \n",
    "model = embedding_dict\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "\n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "\n",
    "# Reshape train and test vectors to 3 dimensions (1 represents one timestep)\n",
    "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 100)            80400     \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                42240     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 122,705\n",
      "Trainable params: 122,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "163/163 [==============================] - 6s 7ms/step - loss: 231.5909 - mae: 8.7269\n",
      "Epoch 2/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 168.3595 - mae: 7.3129\n",
      "Epoch 3/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 139.9834 - mae: 6.7076\n",
      "Epoch 4/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 119.5541 - mae: 6.0472\n",
      "Epoch 5/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 106.5077 - mae: 5.7125\n",
      "Epoch 6/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 100.5295 - mae: 5.5195\n",
      "Epoch 7/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 92.3491 - mae: 5.3104\n",
      "Epoch 8/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 88.6857 - mae: 5.2052\n",
      "Epoch 9/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 84.8085 - mae: 5.1103\n",
      "Epoch 10/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 82.5762 - mae: 5.0395\n",
      "Epoch 11/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 79.4073 - mae: 4.9113\n",
      "Epoch 12/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 75.5335 - mae: 4.8179\n",
      "Epoch 13/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 71.9985 - mae: 4.6968\n",
      "Epoch 14/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 69.4368 - mae: 4.6357\n",
      "Epoch 15/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 66.4807 - mae: 4.5148\n",
      "Epoch 16/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 64.9795 - mae: 4.4906\n",
      "Epoch 17/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 64.8050 - mae: 4.4330\n",
      "Epoch 18/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 60.6301 - mae: 4.3046\n",
      "Epoch 19/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 60.8858 - mae: 4.2938\n",
      "Epoch 20/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 57.8538 - mae: 4.1726\n",
      "Epoch 21/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 55.4945 - mae: 4.1227\n",
      "Epoch 22/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 57.0216 - mae: 4.0988\n",
      "Epoch 23/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 57.1969 - mae: 4.1271\n",
      "Epoch 24/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 56.9028 - mae: 4.0539\n",
      "Epoch 25/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 54.1432 - mae: 3.9513\n",
      "Epoch 26/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 53.0023 - mae: 3.9697\n",
      "Epoch 27/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 52.9441 - mae: 4.0121\n",
      "Epoch 28/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 49.9493 - mae: 3.8758\n",
      "Epoch 29/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 50.9587 - mae: 3.8776\n",
      "Epoch 30/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 49.2033 - mae: 3.8321\n",
      "Epoch 31/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 48.7013 - mae: 3.7949\n",
      "Epoch 32/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 48.5793 - mae: 3.7960\n",
      "Epoch 33/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 46.8198 - mae: 3.7505\n",
      "Epoch 34/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 47.6077 - mae: 3.7435\n",
      "Epoch 35/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 47.8657 - mae: 3.7265\n",
      "Epoch 36/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 46.3876 - mae: 3.6858\n",
      "Epoch 37/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 45.3737 - mae: 3.6609\n",
      "Epoch 38/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 46.7019 - mae: 3.6898\n",
      "Epoch 39/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 46.8622 - mae: 3.6684\n",
      "Epoch 40/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 46.0636 - mae: 3.6508\n",
      "Epoch 41/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 45.4485 - mae: 3.6060\n",
      "Epoch 42/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 45.7124 - mae: 3.6628\n",
      "Epoch 43/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 43.7633 - mae: 3.5804\n",
      "Epoch 44/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 42.3129 - mae: 3.5637\n",
      "Epoch 45/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 44.8013 - mae: 3.6103\n",
      "Epoch 46/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 41.8517 - mae: 3.5212\n",
      "Epoch 47/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 43.9978 - mae: 3.5711\n",
      "Epoch 48/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 42.3473 - mae: 3.5404\n",
      "Epoch 49/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 42.4689 - mae: 3.5309\n",
      "Epoch 50/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 42.6823 - mae: 3.5253\n",
      "Epoch 51/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 41.7720 - mae: 3.4836\n",
      "Epoch 52/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 41.5865 - mae: 3.4981\n",
      "Epoch 53/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 42.2995 - mae: 3.4754\n",
      "Epoch 54/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 40.6583 - mae: 3.4483\n",
      "Epoch 55/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 42.8683 - mae: 3.5243\n",
      "Epoch 56/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 42.2921 - mae: 3.4605\n",
      "Epoch 57/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 39.7993 - mae: 3.3952\n",
      "Epoch 58/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 41.1240 - mae: 3.4416\n",
      "Epoch 59/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 40.5900 - mae: 3.4271\n",
      "Epoch 60/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 39.2852 - mae: 3.4169\n",
      "Epoch 61/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 38.3954 - mae: 3.3904\n",
      "Epoch 62/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 38.3210 - mae: 3.3867\n",
      "Epoch 63/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 40.5167 - mae: 3.4351\n",
      "Epoch 64/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 41.2329 - mae: 3.4446\n",
      "Epoch 65/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 38.0873 - mae: 3.3559\n",
      "Epoch 66/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 40.8240 - mae: 3.4582\n",
      "Epoch 67/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 38.1634 - mae: 3.3205\n",
      "Epoch 68/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 39.0805 - mae: 3.3753\n",
      "Epoch 69/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 41.1916 - mae: 3.4012\n",
      "Epoch 70/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 37.1662 - mae: 3.3363\n",
      "Epoch 71/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 39.2318 - mae: 3.3617\n",
      "Epoch 72/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.2883 - mae: 3.2684\n",
      "Epoch 73/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.6480 - mae: 3.3228\n",
      "Epoch 74/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 38.9090 - mae: 3.3370\n",
      "Epoch 75/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.8776 - mae: 3.3029\n",
      "Epoch 76/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.0621 - mae: 3.2930\n",
      "Epoch 77/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 38.7213 - mae: 3.3274\n",
      "Epoch 78/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.1024 - mae: 3.3200\n",
      "Epoch 79/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 37.9156 - mae: 3.3014\n",
      "Epoch 80/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 37.8479 - mae: 3.3075\n",
      "Epoch 81/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 39.6483 - mae: 3.3048\n",
      "Epoch 82/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 35.9426 - mae: 3.2285\n",
      "Epoch 83/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 37.8509 - mae: 3.2915\n",
      "Epoch 84/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 36.6631 - mae: 3.2572\n",
      "Epoch 85/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 35.7461 - mae: 3.2179\n",
      "Epoch 86/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 34.9026 - mae: 3.2023\n",
      "Epoch 87/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 36.3205 - mae: 3.2691\n",
      "Epoch 88/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 37.2368 - mae: 3.2358\n",
      "Epoch 89/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 34.7722 - mae: 3.1768\n",
      "Epoch 90/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 36.1806 - mae: 3.2265\n",
      "Epoch 91/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 36.3173 - mae: 3.2621\n",
      "Epoch 92/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 35.0008 - mae: 3.1603\n",
      "Epoch 93/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 36.8325 - mae: 3.2319\n",
      "Epoch 94/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 36.6664 - mae: 3.2228\n",
      "Epoch 95/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 34.1647 - mae: 3.1538\n",
      "Epoch 96/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 35.1206 - mae: 3.1806\n",
      "Epoch 97/150\n",
      "163/163 [==============================] - 2s 9ms/step - loss: 35.0883 - mae: 3.1981\n",
      "Epoch 98/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 34.3547 - mae: 3.1395\n",
      "Epoch 99/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 34.3397 - mae: 3.1575\n",
      "Epoch 100/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 33.7263 - mae: 3.1627\n",
      "Epoch 101/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 34.5136 - mae: 3.1362\n",
      "Epoch 102/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 35.5792 - mae: 3.1995\n",
      "Epoch 103/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 36.3281 - mae: 3.1931\n",
      "Epoch 104/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 35.4896 - mae: 3.1978\n",
      "Epoch 105/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 33.7805 - mae: 3.1324\n",
      "Epoch 106/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 34.1331 - mae: 3.1778\n",
      "Epoch 107/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 33.2640 - mae: 3.1197\n",
      "Epoch 108/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 32.8135 - mae: 3.0927\n",
      "Epoch 109/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 33.0433 - mae: 3.1394\n",
      "Epoch 110/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 33.2454 - mae: 3.1061\n",
      "Epoch 111/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 32.8988 - mae: 3.1344\n",
      "Epoch 112/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 34.9531 - mae: 3.1369\n",
      "Epoch 113/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 33.6338 - mae: 3.1006\n",
      "Epoch 114/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 33.1730 - mae: 3.0898\n",
      "Epoch 115/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 32.2853 - mae: 3.0891\n",
      "Epoch 116/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 35.3908 - mae: 3.1778\n",
      "Epoch 117/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 32.2011 - mae: 3.0627\n",
      "Epoch 118/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 31.2571 - mae: 3.0433\n",
      "Epoch 119/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 33.6126 - mae: 3.0935\n",
      "Epoch 120/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 33.2801 - mae: 3.1085\n",
      "Epoch 121/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 32.3932 - mae: 3.0594\n",
      "Epoch 122/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 34.3037 - mae: 3.1330\n",
      "Epoch 123/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 33.5986 - mae: 3.0971\n",
      "Epoch 124/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 31.6472 - mae: 3.0454\n",
      "Epoch 125/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 32.7474 - mae: 3.0565\n",
      "Epoch 126/150\n",
      "163/163 [==============================] - 2s 11ms/step - loss: 31.9012 - mae: 3.0643\n",
      "Epoch 127/150\n",
      "163/163 [==============================] - 2s 10ms/step - loss: 31.4981 - mae: 3.0369\n",
      "Epoch 128/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.8263 - mae: 3.0571\n",
      "Epoch 129/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.4723 - mae: 3.0532\n",
      "Epoch 130/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 33.1231 - mae: 3.0646\n",
      "Epoch 131/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 30.0187 - mae: 3.0104\n",
      "Epoch 132/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 33.4252 - mae: 3.0980\n",
      "Epoch 133/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.8509 - mae: 3.0420\n",
      "Epoch 134/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 32.6549 - mae: 3.0611\n",
      "Epoch 135/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 32.7001 - mae: 3.0602\n",
      "Epoch 136/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 33.0977 - mae: 3.0692\n",
      "Epoch 137/150\n",
      "163/163 [==============================] - 1s 7ms/step - loss: 32.0348 - mae: 3.0173\n",
      "Epoch 138/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 32.2166 - mae: 3.0420\n",
      "Epoch 139/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 29.2354 - mae: 2.9538\n",
      "Epoch 140/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 30.8408 - mae: 3.0343\n",
      "Epoch 141/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 30.5694 - mae: 2.9889\n",
      "Epoch 142/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.3954 - mae: 2.9925\n",
      "Epoch 143/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.7289 - mae: 3.0108\n",
      "Epoch 144/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 31.4773 - mae: 3.0358\n",
      "Epoch 145/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 32.4779 - mae: 3.0438\n",
      "Epoch 146/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 32.5032 - mae: 3.0551\n",
      "Epoch 147/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 29.0927 - mae: 2.9264\n",
      "Epoch 148/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 31.4957 - mae: 3.0264\n",
      "Epoch 149/150\n",
      "163/163 [==============================] - 1s 8ms/step - loss: 32.2574 - mae: 3.0175\n",
      "Epoch 150/150\n",
      "163/163 [==============================] - 1s 9ms/step - loss: 30.2514 - mae: 2.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e6c367340>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = get_model()\n",
    "lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 3ms/step\n",
      "Kappa Score: 0.9575496052479834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "lstm_model.save('./final_lstm_model100.h5')\n",
    "\n",
    "y_pred = lstm_model.predict(testDataVecs)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using 200d Glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_essays = X_train['essay']\n",
    "test_essays = X_test['essay']\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for essay in train_essays:\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "\n",
    "num_features = 200     \n",
    "model = embedding_dict\n",
    "\n",
    "clean_train_essays = []\n",
    "for essay_v in train_essays:\n",
    "    clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "\n",
    "clean_test_essays = []\n",
    "for essay_v in test_essays:\n",
    "    clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_essays, model, num_features)\n",
    "\n",
    "trainDataVecs = np.array(trainDataVecs)\n",
    "testDataVecs = np.array(testDataVecs)\n",
    "\n",
    "# Reshape train and test vectors to 3 dimensions (1 represents one timestep)\n",
    "trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_8 (LSTM)               (None, 1, 200)            320800    \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                67840     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 388,705\n",
      "Trainable params: 388,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/150\n",
      "163/163 [==============================] - 6s 12ms/step - loss: 212.6135 - mae: 8.0394\n",
      "Epoch 2/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 150.5414 - mae: 6.4307\n",
      "Epoch 3/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 116.1396 - mae: 5.4776\n",
      "Epoch 4/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 96.3485 - mae: 5.0393\n",
      "Epoch 5/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 83.1488 - mae: 4.7098\n",
      "Epoch 6/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 77.9320 - mae: 4.5648\n",
      "Epoch 7/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 70.8081 - mae: 4.4414\n",
      "Epoch 8/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 67.3778 - mae: 4.3654\n",
      "Epoch 9/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 59.8610 - mae: 4.1591\n",
      "Epoch 10/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 53.5374 - mae: 3.9535\n",
      "Epoch 11/150\n",
      "163/163 [==============================] - 2s 12ms/step - loss: 47.1827 - mae: 3.7604\n",
      "Epoch 12/150\n",
      "163/163 [==============================] - 2s 13ms/step - loss: 42.9619 - mae: 3.6310\n",
      "Epoch 13/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 41.4834 - mae: 3.5034\n",
      "Epoch 14/150\n",
      "163/163 [==============================] - 5s 28ms/step - loss: 38.7840 - mae: 3.4194\n",
      "Epoch 15/150\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 39.5683 - mae: 3.3994\n",
      "Epoch 16/150\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 36.7191 - mae: 3.3531\n",
      "Epoch 17/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 37.9208 - mae: 3.3552\n",
      "Epoch 18/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 37.2245 - mae: 3.3229\n",
      "Epoch 19/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 32.5361 - mae: 3.2127\n",
      "Epoch 20/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 33.1261 - mae: 3.2000\n",
      "Epoch 21/150\n",
      "163/163 [==============================] - 4s 27ms/step - loss: 33.0817 - mae: 3.1985\n",
      "Epoch 22/150\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 32.6236 - mae: 3.1483\n",
      "Epoch 23/150\n",
      "163/163 [==============================] - 4s 27ms/step - loss: 32.0778 - mae: 3.1352\n",
      "Epoch 24/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 32.3488 - mae: 3.1467\n",
      "Epoch 25/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 30.4633 - mae: 3.0828\n",
      "Epoch 26/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 31.7725 - mae: 3.1037\n",
      "Epoch 27/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 30.8427 - mae: 3.0643\n",
      "Epoch 28/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 30.3891 - mae: 3.0438\n",
      "Epoch 29/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 30.9866 - mae: 3.0683\n",
      "Epoch 30/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 29.4465 - mae: 2.9865\n",
      "Epoch 31/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 28.2334 - mae: 2.9792\n",
      "Epoch 32/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 30.5193 - mae: 3.0299\n",
      "Epoch 33/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 30.1492 - mae: 3.0105\n",
      "Epoch 34/150\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 28.5560 - mae: 2.9581\n",
      "Epoch 35/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 29.3912 - mae: 2.9654\n",
      "Epoch 36/150\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 27.7910 - mae: 2.9158\n",
      "Epoch 37/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 26.9450 - mae: 2.8924\n",
      "Epoch 38/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 28.6817 - mae: 2.9338\n",
      "Epoch 39/150\n",
      "163/163 [==============================] - 4s 25ms/step - loss: 27.9398 - mae: 2.8973\n",
      "Epoch 40/150\n",
      "163/163 [==============================] - 4s 24ms/step - loss: 27.9333 - mae: 2.9016\n",
      "Epoch 41/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 28.3446 - mae: 2.9217\n",
      "Epoch 42/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 26.7878 - mae: 2.8896\n",
      "Epoch 43/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 26.1439 - mae: 2.8442\n",
      "Epoch 44/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 27.0465 - mae: 2.8639\n",
      "Epoch 45/150\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 26.4418 - mae: 2.8371\n",
      "Epoch 46/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 27.2207 - mae: 2.8845\n",
      "Epoch 47/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 26.0847 - mae: 2.8122\n",
      "Epoch 48/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 25.2925 - mae: 2.8116\n",
      "Epoch 49/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 26.3771 - mae: 2.8233\n",
      "Epoch 50/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 27.1543 - mae: 2.8413\n",
      "Epoch 51/150\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 25.5760 - mae: 2.7895\n",
      "Epoch 52/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 27.4760 - mae: 2.8476\n",
      "Epoch 53/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 26.3743 - mae: 2.8042\n",
      "Epoch 54/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 26.8155 - mae: 2.8127\n",
      "Epoch 55/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 25.7419 - mae: 2.7999\n",
      "Epoch 56/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 26.1571 - mae: 2.8231\n",
      "Epoch 57/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 24.2276 - mae: 2.7537\n",
      "Epoch 58/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 24.9761 - mae: 2.7457\n",
      "Epoch 59/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 25.0513 - mae: 2.7668\n",
      "Epoch 60/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 24.0222 - mae: 2.7415\n",
      "Epoch 61/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 24.6737 - mae: 2.7434\n",
      "Epoch 62/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 23.8054 - mae: 2.7086\n",
      "Epoch 63/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 24.9345 - mae: 2.7465\n",
      "Epoch 64/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 22.6201 - mae: 2.6883\n",
      "Epoch 65/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 23.9223 - mae: 2.7402\n",
      "Epoch 66/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 24.2592 - mae: 2.7224\n",
      "Epoch 67/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 23.8096 - mae: 2.6952\n",
      "Epoch 68/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 24.8637 - mae: 2.7324\n",
      "Epoch 69/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 24.5218 - mae: 2.7178\n",
      "Epoch 70/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 23.1868 - mae: 2.6923\n",
      "Epoch 71/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 24.1343 - mae: 2.7075\n",
      "Epoch 72/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 22.9922 - mae: 2.6847\n",
      "Epoch 73/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 22.5064 - mae: 2.6677\n",
      "Epoch 74/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 23.5118 - mae: 2.6716\n",
      "Epoch 75/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 22.5801 - mae: 2.6497\n",
      "Epoch 76/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.9620 - mae: 2.6394\n",
      "Epoch 77/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 23.1400 - mae: 2.6717\n",
      "Epoch 78/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 23.3916 - mae: 2.6257\n",
      "Epoch 79/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.6426 - mae: 2.5877\n",
      "Epoch 80/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 22.8685 - mae: 2.6318\n",
      "Epoch 81/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 22.3594 - mae: 2.6118\n",
      "Epoch 82/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.7627 - mae: 2.5986\n",
      "Epoch 83/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 22.7049 - mae: 2.6331\n",
      "Epoch 84/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 23.4177 - mae: 2.6440\n",
      "Epoch 85/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 22.7057 - mae: 2.6534\n",
      "Epoch 86/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 22.7488 - mae: 2.6177\n",
      "Epoch 87/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.6645 - mae: 2.5943\n",
      "Epoch 88/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 23.3836 - mae: 2.6189\n",
      "Epoch 89/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.0421 - mae: 2.6048\n",
      "Epoch 90/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 20.8427 - mae: 2.5444\n",
      "Epoch 91/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 21.1700 - mae: 2.5811\n",
      "Epoch 92/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.5943 - mae: 2.5877\n",
      "Epoch 93/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.9066 - mae: 2.5818\n",
      "Epoch 94/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.3250 - mae: 2.5505\n",
      "Epoch 95/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.5176 - mae: 2.6002\n",
      "Epoch 96/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 22.2298 - mae: 2.5908\n",
      "Epoch 97/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 20.5654 - mae: 2.5475\n",
      "Epoch 98/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 20.6086 - mae: 2.5548\n",
      "Epoch 99/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.8242 - mae: 2.5710\n",
      "Epoch 100/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 21.1306 - mae: 2.5541\n",
      "Epoch 101/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 21.4408 - mae: 2.5299\n",
      "Epoch 102/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 20.4085 - mae: 2.5178\n",
      "Epoch 103/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 19.9715 - mae: 2.5156\n",
      "Epoch 104/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 22.7918 - mae: 2.5819\n",
      "Epoch 105/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 21.1241 - mae: 2.5268\n",
      "Epoch 106/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 20.6366 - mae: 2.5073\n",
      "Epoch 107/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 20.9648 - mae: 2.5167\n",
      "Epoch 108/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 20.6616 - mae: 2.5136\n",
      "Epoch 109/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 19.3504 - mae: 2.4921\n",
      "Epoch 110/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 20.5457 - mae: 2.5311\n",
      "Epoch 111/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 20.6285 - mae: 2.5162\n",
      "Epoch 112/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 21.1322 - mae: 2.5127\n",
      "Epoch 113/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 20.9260 - mae: 2.5272\n",
      "Epoch 114/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 19.6667 - mae: 2.4687\n",
      "Epoch 115/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 19.8498 - mae: 2.4565\n",
      "Epoch 116/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 20.5707 - mae: 2.5021\n",
      "Epoch 117/150\n",
      "163/163 [==============================] - 4s 22ms/step - loss: 19.7561 - mae: 2.4790\n",
      "Epoch 118/150\n",
      "163/163 [==============================] - 4s 22ms/step - loss: 18.9708 - mae: 2.4332\n",
      "Epoch 119/150\n",
      "163/163 [==============================] - 4s 22ms/step - loss: 20.5221 - mae: 2.4973\n",
      "Epoch 120/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 20.7145 - mae: 2.4832\n",
      "Epoch 121/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 19.2528 - mae: 2.4698\n",
      "Epoch 122/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 18.8179 - mae: 2.4472\n",
      "Epoch 123/150\n",
      "163/163 [==============================] - 3s 21ms/step - loss: 19.6235 - mae: 2.4558\n",
      "Epoch 124/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 19.1145 - mae: 2.4659\n",
      "Epoch 125/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 20.0121 - mae: 2.4872\n",
      "Epoch 126/150\n",
      "163/163 [==============================] - 3s 16ms/step - loss: 18.6027 - mae: 2.4181\n",
      "Epoch 127/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 20.7971 - mae: 2.5157\n",
      "Epoch 128/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 20.2676 - mae: 2.5066\n",
      "Epoch 129/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 19.7382 - mae: 2.4722\n",
      "Epoch 130/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 18.7359 - mae: 2.4482\n",
      "Epoch 131/150\n",
      "163/163 [==============================] - 3s 15ms/step - loss: 19.6670 - mae: 2.4744\n",
      "Epoch 132/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 19.0428 - mae: 2.4697\n",
      "Epoch 133/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 19.5968 - mae: 2.4685\n",
      "Epoch 134/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 18.7218 - mae: 2.4158\n",
      "Epoch 135/150\n",
      "163/163 [==============================] - 2s 14ms/step - loss: 19.4801 - mae: 2.4523\n",
      "Epoch 136/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 18.2072 - mae: 2.4278\n",
      "Epoch 137/150\n",
      "163/163 [==============================] - 2s 15ms/step - loss: 19.2113 - mae: 2.4454\n",
      "Epoch 138/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 18.3638 - mae: 2.4169\n",
      "Epoch 139/150\n",
      "163/163 [==============================] - 3s 20ms/step - loss: 18.7493 - mae: 2.4306\n",
      "Epoch 140/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 18.5418 - mae: 2.4127\n",
      "Epoch 141/150\n",
      "163/163 [==============================] - 3s 17ms/step - loss: 20.1014 - mae: 2.4782\n",
      "Epoch 142/150\n",
      "163/163 [==============================] - 3s 18ms/step - loss: 18.9816 - mae: 2.4381\n",
      "Epoch 143/150\n",
      "163/163 [==============================] - 3s 19ms/step - loss: 18.0529 - mae: 2.3997\n",
      "Epoch 144/150\n",
      "163/163 [==============================] - 4s 22ms/step - loss: 17.7731 - mae: 2.3899\n",
      "Epoch 145/150\n",
      "163/163 [==============================] - 4s 26ms/step - loss: 18.6789 - mae: 2.4139\n",
      "Epoch 146/150\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 18.7729 - mae: 2.4000\n",
      "Epoch 147/150\n",
      "163/163 [==============================] - 4s 27ms/step - loss: 18.2884 - mae: 2.4276\n",
      "Epoch 148/150\n",
      "163/163 [==============================] - 5s 29ms/step - loss: 18.0313 - mae: 2.3776\n",
      "Epoch 149/150\n",
      "163/163 [==============================] - 5s 31ms/step - loss: 17.6631 - mae: 2.3713\n",
      "Epoch 150/150\n",
      "163/163 [==============================] - 6s 38ms/step - loss: 18.5554 - mae: 2.4219\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25e6d959570>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model = get_model()\n",
    "lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 7ms/step\n",
      "Kappa Score: 0.9629401012634522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "lstm_model.save('./final_lstm_model200.h5')\n",
    "\n",
    "y_pred = lstm_model.predict(testDataVecs)\n",
    "y_pred = np.round(y_pred)\n",
    "\n",
    "result = cohen_kappa_score(y_test.values, y_pred, weights='quadratic')\n",
    "print(\"Kappa Score: {}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
